{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import threading\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE():\n",
    "    def __init__(self,vocab_size,args):\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.latent_size = args.latent_size\n",
    "        self.lr = tf.Variable(args.lr, trainable=False)\n",
    "        self.num_prop = args.num_prop\n",
    "        self.stddev = args.stddev\n",
    "        self.mean = args.mean\n",
    "        self.unit_size = args.unit_size\n",
    "        self.n_rnn_layer = args.n_rnn_layer\n",
    "        \n",
    "        self._create_network()\n",
    "\n",
    "\n",
    "    def _create_network(self):\n",
    "        self.X = tf.placeholder(tf.int32, [self.batch_size, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [self.batch_size, None])\n",
    "        self.C = tf.placeholder(tf.float32, [self.batch_size, self.num_prop])\n",
    "        self.L = tf.placeholder(tf.int32, [self.batch_size])\n",
    "        \n",
    "\n",
    "        \n",
    "        decoded_rnn_size = [self.unit_size for i in range(self.n_rnn_layer)]\n",
    "        encoded_rnn_size = [self.unit_size for i in range(self.n_rnn_layer)]\n",
    "        \n",
    "        with tf.variable_scope('decode'):\n",
    "            decode_cell=[]\n",
    "            for i in decoded_rnn_size[:]:\n",
    "                decode_cell.append(tf.nn.rnn_cell.LSTMCell(i))\n",
    "            self.decoder = tf.nn.rnn_cell.MultiRNNCell(decode_cell)\n",
    "        \n",
    "        with tf.variable_scope('encode'):\n",
    "            encode_cell=[]\n",
    "            for i in encoded_rnn_size[:]:\n",
    "                encode_cell.append(tf.nn.rnn_cell.LSTMCell(i))\n",
    "            self.encoder = tf.nn.rnn_cell.MultiRNNCell(encode_cell)\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        self.eps = {\n",
    "            'eps' : tf.random_normal([self.batch_size, self.latent_size], stddev=self.stddev, mean=self.mean)\n",
    "        }\n",
    "\n",
    "\n",
    "        self.weights['softmax'] = tf.get_variable(\"softmaxw\", initializer=tf.random_uniform(shape=[decoded_rnn_size[-1], self.vocab_size], minval = -0.1, maxval = 0.1))       \n",
    "        \n",
    "        self.biases['softmax'] =  tf.get_variable(\"softmaxb\", initializer=tf.zeros(shape=[self.vocab_size]))\n",
    "        self.weights['out_mean'] = tf.get_variable(\"outmeanw\", initializer=tf.contrib.layers.xavier_initializer(), shape=[self.unit_size, self.latent_size]),\n",
    "        self.weights['out_log_sigma'] = tf.get_variable(\"outlogsigmaw\", initializer=tf.contrib.layers.xavier_initializer(), shape=[self.unit_size, self.latent_size]),\n",
    "        self.biases['out_mean'] = tf.get_variable(\"outmeanb\", initializer=tf.zeros_initializer(), shape=[self.latent_size]),\n",
    "        self.biases['out_log_sigma'] = tf.get_variable(\"outlogsigmab\", initializer=tf.zeros_initializer(), shape=[self.latent_size]),\n",
    "\n",
    "        self.embedding_encode = tf.get_variable(name = 'encode_embedding', shape = [self.latent_size, self.vocab_size], initializer = tf.random_uniform_initializer( minval = -0.1, maxval = 0.1))\n",
    "        self.embedding_decode = tf.get_variable(name = 'decode_embedding', shape = [self.latent_size, self.vocab_size], initializer = tf.random_uniform_initializer( minval = -0.1, maxval = 0.1))\n",
    "        \n",
    "        self.latent_vector, self.mean, self.log_sigma = self.encode()\n",
    "\n",
    "        self.decoded, decoded_logits = self.decode(self.latent_vector)\n",
    "        #self.Y_generated = self.generate()\n",
    "\n",
    "        weights = tf.sequence_mask(self.L, tf.shape(self.X)[1])\n",
    "        weights = tf.cast(weights, tf.int32)\n",
    "        weights = tf.cast(weights, tf.float32)\n",
    "        self.reconstr_loss = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(\n",
    "            logits=decoded_logits, targets=self.Y, weights=weights))\n",
    "        self.latent_loss = self.cal_latent_loss(self.mean, self.log_sigma)\n",
    "\n",
    "        # Loss\n",
    "\n",
    "        self.loss = self.latent_loss + self.reconstr_loss \n",
    "        #self.loss = self.reconstr_loss \n",
    "        optimizer    = tf.train.AdamOptimizer(self.lr)\n",
    "        self.opt = optimizer.minimize(self.loss)\n",
    "        \n",
    "        self.mol_pred = tf.argmax(self.decoded, axis=2)\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        self.saver = tf.train.Saver(max_to_keep=None)\n",
    "        #tf.train.start_queue_runners(sess=self.sess)\n",
    "        print (\"Network Ready\")\n",
    "\n",
    "    def encode(self): \n",
    "        X = tf.nn.embedding_lookup(self.embedding_encode, self.X)\n",
    "        C = tf.expand_dims(self.C, 1)\n",
    "        C = tf.tile(C, [1, tf.shape(X)[1], 1])\n",
    "        inp = tf.concat([X, C], axis=-1)\n",
    "        _, state = tf.nn.dynamic_rnn(self.encoder, inp, dtype=tf.float32, scope = 'encode', sequence_length = self.L)\n",
    "        c,h = state[-1]\n",
    "        self.weights['out_mean'] = tf.reshape(self.weights['out_mean'], [self.unit_size, -1])\n",
    "        self.weights['out_log_sigma'] = tf.reshape(self.weights['out_log_sigma'], [self.unit_size, -1])\n",
    "        mean = tf.matmul(h, self.weights['out_mean'])+self.biases['out_mean']\n",
    "        log_sigma = tf.matmul(h, self.weights['out_log_sigma'])+self.biases['out_log_sigma']\n",
    "        retval = mean+tf.exp(log_sigma/2.0)*self.eps['eps']\n",
    "        return retval, mean, log_sigma\n",
    "\n",
    "    def decode(self, Z):\n",
    "        seq_length=tf.shape(self.X)[1]\n",
    "        new_Z = tf.tile(tf.expand_dims(Z, 1), [1, seq_length, 1])\n",
    "        C = tf.expand_dims(self.C, 1)\n",
    "        C = tf.tile(C, [1, tf.shape(self.X)[1], 1])\n",
    "        X = tf.nn.embedding_lookup(self.embedding_encode, self.X)\n",
    "        inputs = tf.concat([new_Z, X, C], axis=-1)\n",
    "        self.initial_decoded_state = tuple([tf.contrib.rnn.LSTMStateTuple(tf.zeros((self.batch_size, self.unit_size)), tf.zeros((self.batch_size, self.unit_size))) for i in range(3)])\n",
    "        #self.initial_decoded_state=self.decoder.zero_state() \n",
    "        Y, self.output_decoded_state = tf.nn.dynamic_rnn(self.decoder, inputs, dtype=tf.float32, scope = 'decode', sequence_length = self.L, initial_state=self.initial_decoded_state)\n",
    "        Y = tf.reshape(Y, [self.batch_size*seq_length, -1])\n",
    "        Y = tf.matmul(Y, self.weights['softmax'])+self.biases['softmax']\n",
    "        Y_logits = tf.reshape(Y, [self.batch_size, seq_length, -1])\n",
    "        Y = tf.nn.softmax(Y_logits)\n",
    "        return Y, Y_logits\n",
    "\n",
    "    def save(self, ckpt_path, global_step):\n",
    "        self.saver.save(self.sess, ckpt_path, global_step = global_step)\n",
    "        #print(\"model saved to '%s'\" % (ckpt_path))\n",
    "\n",
    "    def assign_lr(self, learning_rate):\n",
    "        self.sess.run(tf.assign(self.lr, learning_rate ))\n",
    "    \n",
    "    def restore(self, ckpt_path):\n",
    "        self.saver.restore(self.sess, ckpt_path)\n",
    "\n",
    "    def get_latent_vector(self, x, c, l):\n",
    "        return self.sess.run(self.latent_vector, feed_dict={self.X : x, self.C : c, self.L : l})\n",
    "\n",
    "    def cal_latent_loss(self, mean, log_sigma):\n",
    "        latent_loss = tf.reduce_mean(-0.5*(1+log_sigma-tf.square(mean)-tf.exp(log_sigma)))\n",
    "        return latent_loss\n",
    "    \n",
    "    def train(self, x, y, l, c):\n",
    "        _, r_loss, l_loss = self.sess.run([self.opt, self.reconstr_loss, self.latent_loss], feed_dict = {self.X :x, self.Y:y, self.L : l, self.C : c})\n",
    "        return r_loss + l_loss\n",
    "    \n",
    "    def test(self, x, y, l, c):\n",
    "        mol_pred, r_loss, l_loss  = self.sess.run([self.mol_pred, self.reconstr_loss, self.latent_loss], feed_dict = {self.X :x, self.Y:y, self.L : l, self.C : c})\n",
    "        return r_loss + l_loss\n",
    "\n",
    "    def sample(self, latent_vector, c, start_codon, seq_length):\n",
    "        l = np.ones((self.batch_size)).astype(np.int32)\n",
    "        x=start_codon\n",
    "        preds = []\n",
    "        for i in range(seq_length):\n",
    "            if i==0:\n",
    "                x, state = self.sess.run([self.mol_pred, self.output_decoded_state], feed_dict = {self.X:x, self.latent_vector:latent_vector, self.L : l, self.C : c})\n",
    "            else:\n",
    "                x, state = self.sess.run([self.mol_pred, self.output_decoded_state], feed_dict = {self.X:x, self.latent_vector:latent_vector, self.L : l, self.C : c, self.initial_decoded_state:state})\n",
    "            preds.append(x)\n",
    "        return np.concatenate(preds,1).astype(int).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py\n",
    "\n",
    "\n",
    "def convert_to_smiles(vector, char):\n",
    "    list_char = list(char)\n",
    "    #list_char = char.tolist()\n",
    "    vector = vector.astype(int)\n",
    "    return \"\".join(map(lambda x: list_char[x], vector)).strip()\n",
    "\n",
    "def stochastic_convert_to_smiles(vector, char):\n",
    "    list_char = char.tolist()\n",
    "    s = \"\"\n",
    "    for i in range(len(vector)):\n",
    "        prob = vector[i].tolist()\n",
    "        norm0 = sum(prob)\n",
    "        prob = [i/norm0 for i in prob]\n",
    "        index = np.random.choice(len(list_char), 1, p=prob)\n",
    "        s+=list_char[index[0]]\n",
    "    return s\n",
    "\n",
    "def one_hot_array(i, n):\n",
    "    return list(map(int, [ix == i for ix in range(n)]))\n",
    "\n",
    "def one_hot_index(vec, charset):\n",
    "    return list(map(charset.index, vec))\n",
    "\n",
    "def from_one_hot_array(vec):\n",
    "    oh = np.where(vec == 1)\n",
    "    if oh[0].shape == (0, ):\n",
    "        return None\n",
    "    return int(oh[0][0])\n",
    "\n",
    "def decode_smiles_from_indexes(vec, charset):\n",
    "    return \"\".join(map(lambda x: charset[x], vec)).strip()\n",
    "\n",
    "def load_dataset(filename, split = True):\n",
    "    h5f = h5py.File(filename, 'r')\n",
    "    if split:\n",
    "        data_train = h5f['data_train'][:]\n",
    "    else:\n",
    "        data_train = None\n",
    "    data_test = h5f['data_test'][:]\n",
    "    charset = h5f['charset'][:]\n",
    "    h5f.close()\n",
    "    if split:\n",
    "        return data_train, data_test, charset\n",
    "    else:\n",
    "        return data_test, charset\n",
    "\n",
    "def encode_smiles(smiles, model, charset):\n",
    "    cropped = list(smiles.ljust(120))\n",
    "    preprocessed = np.array([list(map(lambda x: one_hot_array(x, len(charset)), one_hot_index(cropped, charset)))])\n",
    "    latent = model.encoder.predict(preprocessed)\n",
    "    return latent\n",
    "\n",
    "def smiles_to_onehot(smiles, charset):\n",
    "    cropped = list(smiles.ljust(120))\n",
    "    preprocessed = np.array([list(map(lambda x: one_hot_array(x, len(charset)), one_hot_index(cropped, charset)))])\n",
    "    return preprocessed\n",
    "\n",
    "def smiles_to_vector(smiles, vocab, max_length):\n",
    "    while len(smiles)<max_length:\n",
    "        smiles +=\" \"\n",
    "    return [vocab.index(str(x)) for x in smiles]\n",
    "\n",
    "def decode_latent_molecule(latent, model, charset, latent_dim):\n",
    "    decoded = model.decoder.predict(latent.reshape(1, latent_dim)).argmax(axis=2)[0]\n",
    "    smiles = decode_smiles_from_indexes(decoded, charset)\n",
    "    return smiles\n",
    "\n",
    "def interpolate(source_smiles, dest_smiles, steps, charset, model, latent_dim):\n",
    "    source_latent = encode_smiles(source_smiles, model, charset)\n",
    "    dest_latent = encode_smiles(dest_smiles, model, charset)\n",
    "    step = (dest_latent - source_latent) / float(steps)\n",
    "    results = []\n",
    "    for i in range(steps):\n",
    "        item = source_latent + (step * i)        \n",
    "        decoded = decode_latent_molecule(item, model, charset, latent_dim)\n",
    "        results.append(decoded)\n",
    "    return results\n",
    "\n",
    "def get_unique_mols(mol_list):\n",
    "    inchi_keys = [Chem.InchiToInchiKey(Chem.MolToInchi(m)) for m in mol_list]\n",
    "    u, indices = np.unique(inchi_keys, return_index=True)\n",
    "    unique_mols = [[mol_list[i], inchi_keys[i]] for i in indices]\n",
    "    return unique_mols\n",
    "\n",
    "def accuracy(arr1, arr2, length):\n",
    "    total = len(arr1)\n",
    "    count1=0\n",
    "    count2=0\n",
    "    count3=0\n",
    "    for i in range(len(arr1)):\n",
    "        if np.array_equal(arr1[i,:length[i]], arr2[i,:length[i]]):\n",
    "            count1+=1\n",
    "    for i in range(len(arr1)):\n",
    "        for j in range(length[i]):\n",
    "            if arr1[i][j]==arr2[i][j]:\n",
    "                count2+=1\n",
    "            count3+=1\n",
    "\n",
    "    return float(count1/float(total)), float(count2/count3)\n",
    "\n",
    "def load_data(n, seq_length):\n",
    "    import collections\n",
    "    f = open(n)\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "    lines = [l.split() for l in lines]\n",
    "    lines = [l for l in lines if len(l[0])<seq_length-2]\n",
    "    smiles = [l[0] for l in lines]\n",
    "    \n",
    "    total_string = ''\n",
    "    for s in smiles:\n",
    "        total_string+=s\n",
    "    counter = collections.Counter(total_string)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    chars, counts = zip(*count_pairs)\n",
    "    vocab = dict(zip(chars, range(len(chars))))\n",
    "\n",
    "    chars+=('E',) #End of smiles\n",
    "    chars+=('X',) #Start of smiles\n",
    "    vocab['E'] = len(chars)-2\n",
    "    vocab['X'] = len(chars)-1\n",
    "    \n",
    "    length = np.array([len(s)+1 for s in smiles])\n",
    "    smiles_input = [('X'+s).ljust(seq_length, 'E') for s in smiles] \n",
    "    smiles_output = [s.ljust(seq_length, 'E') for s in smiles] \n",
    "    smiles_input = np.array([np.array(list(map(vocab.get, s)))for s in smiles_input])\n",
    "    smiles_output = np.array([np.array(list(map(vocab.get, s)))for s in smiles_output])\n",
    "    prop = np.array([l[1:] for l in lines])\n",
    "    return smiles_input, smiles_output, chars, vocab, prop, length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка конфигурации из JSON-файла\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Путь к файлу конфигурации\n",
    "config_path = r'C:\\Users\\Igorr\\Documents\\ITMO5grade\\Project_with_Susan\\Made_code_github\\Made_code\\Learning\\config_train.json'\n",
    "\n",
    "# Загружаем параметры\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Распаковываем параметры из конфигурации\n",
    "batch_size = config.get('batch_size', 128)\n",
    "latent_size = config.get('latent_size', 200)\n",
    "unit_size = config.get('unit_size', 512)\n",
    "n_rnn_layer = config.get('n_rnn_layer', 3)\n",
    "seq_length = config.get('seq_length', 120)\n",
    "prop_file = config.get('prop_file', None)\n",
    "mean = config.get('mean', 0.0)\n",
    "stddev = config.get('stddev', 1.0)\n",
    "num_epochs = config.get('num_epochs', 100)\n",
    "lr = config.get('lr', 0.0001)\n",
    "num_prop = config.get('num_prop', 3)\n",
    "save_dir = config.get('save_dir', 'save/')\n",
    "\n",
    "# Проверяем обязательные параметры\n",
    "if prop_file is None:\n",
    "    raise ValueError(\"Parameter 'prop_file' is required in the config.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
